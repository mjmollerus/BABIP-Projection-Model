{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped data from two sources, a large variety of statistics from Fangraphs and batted ball profile and shift\n",
    "statistics from Baseball Savant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyforest import *\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fangraphs Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to scrape the data any player who had had a qualified season between 2014 and 2019, so first I ran through the Fangraphs leaderboards to collect those players' names then scraped their statitistics. There were numerous issues that complicated scraping, including the formatting statistics for players who had changed teams mid-season and missing data; the solutions are outlined in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFGPlayerPages(urls):\n",
    "    \n",
    "    '''Takes in list of urls of FG leaderboards and returns list of urls of desired player pages'''\n",
    "    \n",
    "    player_pages=[]\n",
    "    \n",
    "    #get leaderboard html for each page\n",
    "    for url in urls:\n",
    "        response=requests.get(url)\n",
    "        allplayerspage = response.text\n",
    "        allplayerssoup = BeautifulSoup(allplayerspage)\n",
    "        \n",
    "        #get url endings and put in list\n",
    "        playertable = allplayerssoup.find('table',id=\"LeaderBoard1_dg1_ctl00\").find('tbody').find_all('tr')\n",
    "        pageurls = []\n",
    "        for row in playertable:\n",
    "            player_pages.append(row.find('a').get('href'))\n",
    "    \n",
    "    return player_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFGPlayerStats(playerurls):\n",
    "    \n",
    "    '''Takes in list of player urls and returns dataframe of the desired statistics'''\n",
    "    \n",
    "    #create columns\n",
    "    cols = ['Year','Player','Team','Games','PA','SB','ISO','Babip','CS','Spd','GB/FB',\n",
    "            'LD','GB','FB','IFFB','HR/FB','IFH','BUH','Pull','Cent','Oppo','Soft','Med',\n",
    "            'Hard','O-Swing','Z-Swing','Swing','O-Contact','Z-Contact','Contact','Zone']\n",
    "    \n",
    "    #create list to collect player-year dicts\n",
    "    dictlist=[]\n",
    "\n",
    "    for i in range(len(playerurls)):\n",
    "        \n",
    "        #create player page soup\n",
    "        response=requests.get(f'https://www.fangraphs.com/{playerurls[i]}')\n",
    "        playerpage=response.text\n",
    "        playersoup=BeautifulSoup(playerpage)\n",
    "\n",
    "        #iterate through years\n",
    "        for year in range(2014,2019):\n",
    "            #create list for each player-year\n",
    "            entry = []\n",
    "            #create counters to prevent there from being repeat for multiple teams cases\n",
    "            counter1,counter2,counter3,counter4,counter5 = 0,0,0,0,0\n",
    "\n",
    "            #iterate through rows in first table\n",
    "            for row in playersoup.find('div',id='SeasonStats1_dgSeason11').find_all('tr'):\n",
    "                #check existence, year, majors, counter\n",
    "                if (len(row.find_all('td')) == 0 or\n",
    "                    not row.find_all('td')[0].find('a') or\n",
    "                    row.find_all('td')[0].find('a').text != str(year) or\n",
    "                    '(' in row.find_all('td')[1].text or\n",
    "                    counter1 == year):\n",
    "                    continue\n",
    "\n",
    "                #append year to babip\n",
    "                else:\n",
    "                    entry.append(year)\n",
    "                    entry.append(playersoup.find('div',id='content').find('h1').text)\n",
    "                    entry.append(row.find_all('td')[1].text)\n",
    "                    entry.append(int(row.find_all('td')[2].text))\n",
    "                    entry.append(int(row.find_all('td')[3].text))\n",
    "                    entry.append(int(row.find_all('td')[7].text))\n",
    "                    entry.append(float(row.find_all('td')[10].text))\n",
    "                    entry.append(float(row.find_all('td')[11].text))\n",
    "                    counter1 = year\n",
    "\n",
    "            #iterate through rows in second table\n",
    "            for row in playersoup.find('div',id='SeasonStats1_dgSeason1').find_all('tr'):\n",
    "                #check existence, year, majors, counter\n",
    "                if (len(row.find_all('td')) == 0 or\n",
    "                    not row.find_all('td')[0].find('a') or\n",
    "                    row.find_all('td')[0].find('a').text != str(year) or\n",
    "                    '(' in row.find_all('td')[1].text or\n",
    "                    counter2 == year):\n",
    "                    continue\n",
    "\n",
    "                #append caught stealing\n",
    "                else:\n",
    "                    entry.append(int(row.find_all('td')[20].text))\n",
    "                    counter2 = year\n",
    "\n",
    "            #iterate through rows in third table\n",
    "            for row in playersoup.find('div',id='SeasonStats1_dgSeason2').find_all('tr'):\n",
    "                #check existence, year, majors, counter\n",
    "                if (len(row.find_all('td')) == 0 or\n",
    "                    not row.find_all('td')[0].find('a') or\n",
    "                    row.find_all('td')[0].find('a').text != str(year) or\n",
    "                    '(' in row.find_all('td')[1].text or \n",
    "                    counter3 == year):\n",
    "                    continue\n",
    "\n",
    "                #append Spd\n",
    "                else:\n",
    "                    entry.append(float(row.find_all('td')[10].text))\n",
    "                    counter3 = year\n",
    "\n",
    "            #iterate through rows in fourth table\n",
    "            for row in playersoup.find('div',id='SeasonStats1_dgSeason3').find_all('tr'):\n",
    "                #check existence, year, majors, counter\n",
    "                if (len(row.find_all('td')) == 0 or\n",
    "                    not row.find_all('td')[0].find('a') or\n",
    "                    row.find_all('td')[0].find('a').text != str(year) or\n",
    "                    '(' in row.find_all('td')[1].text or \n",
    "                    counter4 == year):\n",
    "                    continue\n",
    "\n",
    "                #append batted ball data\n",
    "                else:\n",
    "                    entry.append(float(row.find_all('td')[2].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[3].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[4].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[5].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[6].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[7].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[8].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[9].text.split()[0]))\n",
    "                    #check if there's not batted ball data, and if its missing input NaN\n",
    "                    if not row.find_all('td')[10].text.isspace():\n",
    "                        entry.append(float(row.find_all('td')[10].text.split()[0]))\n",
    "                        entry.append(float(row.find_all('td')[11].text.split()[0]))\n",
    "                        entry.append(float(row.find_all('td')[12].text.split()[0]))\n",
    "                        entry.append(float(row.find_all('td')[13].text.split()[0]))\n",
    "                        entry.append(float(row.find_all('td')[14].text.split()[0]))\n",
    "                        entry.append(float(row.find_all('td')[15].text.split()[0]))\n",
    "                    else:\n",
    "                        for i in range(1,7):\n",
    "                            entry.append(np.NaN)                    \n",
    "                    counter4 = year\n",
    "\n",
    "            #iterate through fifth table\n",
    "            for row in playersoup.find('div',id='SeasonStats1_dgSeason7').find_all('tr'):\n",
    "                #check existence, year, majors, counter\n",
    "                if (len(row.find_all('td')) == 0 or\n",
    "                    not row.find_all('td')[0].find('a') or\n",
    "                    row.find_all('td')[0].find('a').text != str(year) or\n",
    "                    '(' in row.find_all('td')[1].text or\n",
    "                    counter5 == year):\n",
    "                    continue\n",
    "\n",
    "                #append plate discipline data\n",
    "                else:\n",
    "                    entry.append(float(row.find_all('td')[2].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[3].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[4].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[5].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[6].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[7].text.split()[0]))\n",
    "                    entry.append(float(row.find_all('td')[8].text.split()[0]))\n",
    "                    counter5 = year\n",
    "            \n",
    "            #append entry to playerdf\n",
    "            entrydict=OrderedDict(zip(cols,entry))\n",
    "            dictlist.append(entrydict)\n",
    "\n",
    "    return pd.DataFrame(dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_urls = ['https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=1_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=2_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=3_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=4_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=5_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=6_50',\n",
    "                    'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2018&month=0&season1=2014&ind=0&team=&rost=&age=&filter=&players=&startdate=&enddate=&page=7_50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_urls = getFGPlayerPages(leaderboard_urls)\n",
    "FG_df = getFGPlayerStats(player_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseball Savant Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I wanted to get exit velocity data from Baseball Savant. To do this I found it easiest just to grab the entire JSON file from which the webpage was generating tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageSoups(urls):\n",
    "    pageSoups = []\n",
    "\n",
    "    for url in urls:\n",
    "        response=requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        pageSoups.append(soup)\n",
    "    return pageSoups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEVJsons(soups):\n",
    "    dictlist = []\n",
    "    \n",
    "    for soup in soups:\n",
    "        jsontext = str(soup.find_all('script')[9]).split('var leaderboard_data = [')[1].split(']')[0]\n",
    "        individuals = jsontext.split(',{')\n",
    "        for i in range(len(individuals)):\n",
    "            if i == 0:\n",
    "                data = re.sub(r'(,\"href\"(.*)a>\")*','',individuals[i])\n",
    "            else:\n",
    "                data = '{' + re.sub(r'(,\"href\"(.*)a>\")*','',individuals[i])\n",
    "            s=json.loads(data)\n",
    "            dictlist.append(s)\n",
    "    \n",
    "    return pd.DataFrame(dictlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar process for data on sprint speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSJsons(soups):\n",
    "    dictlist = []\n",
    "    \n",
    "    for soup in soups:\n",
    "        jsontext = str(soup.find_all('script')[9].text.split('[')[1].split(']')[0])\n",
    "        individual = jsontext.split(',{')\n",
    "        for i in range(len(individual)):\n",
    "            if i == 0:\n",
    "                data = re.sub(r'(,\"href\"(.*)a>\")*','',individual[i])\n",
    "            else:\n",
    "                data = '{' + re.sub(r'(,\"href\"(.*)a>\")*','',individual[i])\n",
    "            s=json.loads(data)\n",
    "            dictlist.append(s)\n",
    "    \n",
    "    return pd.DataFrame(dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShiftData(url):\n",
    "    \n",
    "    response=requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml') \n",
    "    \n",
    "    cols = ['Player','Year','Shift%']\n",
    "    dictlist = []\n",
    "    \n",
    "    row_list = soup.find('table',id='search_results').find('tbody').find_all('tr',class_='search_row')\n",
    "    \n",
    "    for i in range(0,len(row_list)):\n",
    "        entry = []\n",
    "        entry.append(row_list[i].find('td',class_='player_name').text)\n",
    "        entry.append(row_list[i].find_all('td')[2].text.split('- ')[1])\n",
    "        entry.append(float(row_list[i].find_all('td')[4].text))\n",
    "        \n",
    "        dictlist.append(OrderedDict(zip(cols,entry)))\n",
    "    \n",
    "    return pd.DataFrame(dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_urls = ['https://baseballsavant.mlb.com/statcast_leaderboard?year=2019&abs=50&player_type=resp_batter_id',\n",
    "          'https://baseballsavant.mlb.com/statcast_leaderboard?year=2018&abs=50&player_type=resp_batter_id',\n",
    "          'https://baseballsavant.mlb.com/statcast_leaderboard?year=2017&abs=50&player_type=resp_batter_id',\n",
    "          'https://baseballsavant.mlb.com/statcast_leaderboard?year=2016&abs=50&player_type=resp_batter_id',\n",
    "          'https://baseballsavant.mlb.com/statcast_leaderboard?year=2015&abs=50&player_type=resp_batter_id']\n",
    "soups = getPageSoups(urls)\n",
    "EV_df = getJsons(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_urls = ['https://baseballsavant.mlb.com/sprint_speed_leaderboard?year=2019&position=&team=&min=10',\n",
    "          'https://baseballsavant.mlb.com/sprint_speed_leaderboard?year=2018&position=&team=&min=10',\n",
    "          'https://baseballsavant.mlb.com/sprint_speed_leaderboard?year=2017&position=&team=&min=10',\n",
    "          'https://baseballsavant.mlb.com/sprint_speed_leaderboard?year=2016&position=&team=&min=10',\n",
    "          'https://baseballsavant.mlb.com/sprint_speed_leaderboard?year=2015&position=&team=&min=10']\n",
    "soups = getPageSoups(urls)\n",
    "SS_df = getJsons(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_url = 'https://baseballsavant.mlb.com/statcast_search?hfPT=&hfAB=&hfBBT=&hfPR=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7C&hfC=&hfSea=2019%7C2018%7C2017%7C2016%7C2015%7C&hfSit=&player_type=batter&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=&game_date_gt=&game_date_lt=&hfInfield=2%7C3%7C&team=&position=&hfOutfield=&hfRO=&home_road=&hfFlag=&hfPull=&metric_1=&hfInn=&min_pitches=0&min_results=0&group_by=name-year&sort_col=pitches&player_event_sort=h_launch_speed&sort_order=desc&min_pas=30'\n",
    "shift_df = getShiftData(shift_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
